{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54832c99",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "authors:\n",
    "  - name: \"Max Jones\"\n",
    "    affiliations:\n",
    "      - \"Development Seed\"\n",
    "    email: \"max@developmentseed.org\"\n",
    "    orcid: \"0000-0003-0180-8928\"\n",
    "    github: \"maxrjones\"\n",
    "---\n",
    "\n",
    "# â˜ï¸ 4 - Data in the Cloud 101\n",
    "\n",
    ":::{tip} ğŸ§­ Where we are going\n",
    ":icon: false\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "- Explain the key benefits and pitfalls of working with data on the Cloud\n",
    "- Open cloud hosted data in a performant way\n",
    "- Find communities to learn more about cloud native science\n",
    "- **Compare performance between local and cloud-based computing**\n",
    ":::\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is for the workshop ([Open Source Geospatial Workflows in the Cloud](https://geojupyter.github.io/workshop-open-source-geospatial)) presented at the [AGU Fall Meeting 2025](https://agu.confex.com/agu/agu25/meetingapp.cgi/Session/252640).\n",
    "\n",
    "### ğŸ†• Local vs. Cloud Comparison Feature\n",
    "\n",
    "This notebook supports running in both **local** and **cloud** environments to demonstrate the performance benefits of \"data-proximate computing\". Run it once locally, then again on cloud infrastructure (e.g., AWS us-west-2) to see the dramatic difference in performance when your compute is co-located with your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da0fe3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What is the cloud?\n",
    "\n",
    "The cloud (as defined by [cloudflare](https://www.cloudflare.com/)) is a distributed collection of servers that host software and infrastructure, and it is accessed over the Internet. The map below (from [salesforce](https://trailhead.salesforce.com/content/learn/modules/aws-cloud-technical-professionals/explore-the-aws-global-infrastructure-technical-professionals)) shows [Amazon Web Service (AWS)](https://aws.amazon.com)'s global distribution of data centers, which contain the resources that make up the AWS cloud. Three large cloud providers in the United States are [AWS](https://aws.amazon.com/), [Google Cloud Platform](https://cloud.google.com/), and [Microsoft Azure](https://azure.microsoft.com/en-us), but other cloud providers are larger elsewhere in the world and there are numerous smaller providers available.\n",
    "\n",
    "![AWS regions](./aws_regions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad673655",
   "metadata": {},
   "source": [
    "## What makes data on the cloud different?\n",
    "\n",
    "Hosting data on the cloud differs from storing data locally (or on-premises) in a few important ways:\n",
    "\n",
    "- Redundancy - you can easily replicate your data across multiple servers, which may be distributed across the globe\n",
    "- Reliability - cloud providers offer services for reliability, such as automated backups and recovery\n",
    "- Scalability - cloud object storage enables nearly limitless simultaneous access across users/connections, without needing to order or decommission servers or hard-drives\n",
    "- Accessibility - anyone in the world, with proper authorization, can rapidly access data shared on the cloud\n",
    "\n",
    "Gotchas: There are a couple of considerations to be aware of when working with data on the cloud:\n",
    "\n",
    "- Pay-as-you-go - Most cloud providers use pay-as-you-go pricing, where you only pay for the storage and services that you use. This can potentially reduce costs, especially upfront costs (e.g., you never need to buy a hard drive). However, **you may want to provide indefinite access or you may forget about data in storage, in both cases you may end up continuing to pay for data storage indefinitely**.\n",
    "- Time and cost of bringing data to your computer - Hosting the data on the cloud naturally means it's no longer already near your computer's processing resources. Transporting data from the cloud to your computer is expensive, since most cloud providers charge for any data leaving their network, and slow, since the data needs to travel large distances. \n",
    "\n",
    "The primary solution for the second bullet, \"time and cost bringing data to your computer\", is \"data-proximate computing\" which involves running your code on computing resources in the same cloud location as your data. For example, I commonly use NASA data products that are hosted on AWS servers in the 'us-west-2' region, which corresponds to Oregon in the figure above. Following the \"data-proximate computing\" paradigm, I use AWS compute resources that are also in Oregon when working with those data, rather than downloading data to use the computing resources on my laptop in North Carolina. In addition to \"data-proximate computing\", there are many other ways to make working with data on the cloud cheaper and easier. Let's take a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f73e5",
   "metadata": {},
   "source": [
    "## What is cloud-native data?\n",
    "\n",
    "Cloud-native data are structured for efficient querying across a network. For this 101 tutorial, you can think of \"a network\" as synonymous with \"the internet\". You can learn more about these data in the [CNG data formats guide](https://guide.cloudnativegeo.org/), but here we'll just explore working with data that is, compared to data that isn't, optimized for cloud usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6efe60",
   "metadata": {},
   "source": [
    "### Setup and Helper Functions\n",
    "\n",
    "First import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4d349c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T22:42:09.041332Z",
     "iopub.status.busy": "2025-12-17T22:42:09.041162Z",
     "iopub.status.idle": "2025-12-17T22:42:09.642647Z",
     "shell.execute_reply": "2025-12-17T22:42:09.641884Z",
     "shell.execute_reply.started": "2025-12-17T22:42:09.041313Z"
    }
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from time import perf_counter\n",
    "import time  # for timestamps only\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\n",
    "  \"ignore\",\n",
    "  message=\"Numcodecs codecs are not in the Zarr version 3 specification*\",\n",
    "  category=UserWarning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f650d4",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "Configure whether you're running locally or on cloud infrastructure. This notebook will save timings to a JSON file so you can compare results from different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84534bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T22:42:09.643849Z",
     "iopub.status.busy": "2025-12-17T22:42:09.643449Z",
     "iopub.status.idle": "2025-12-17T22:42:09.647526Z",
     "shell.execute_reply": "2025-12-17T22:42:09.646836Z",
     "shell.execute_reply.started": "2025-12-17T22:42:09.643827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸  Running in 'CLOUD' environment\n",
      "ğŸ“ Description: CryoCloud\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT CONFIGURATION - MODIFY THIS FOR YOUR RUN\n",
    "# =============================================================================\n",
    "\n",
    "# Set to \"local\" when running on your local machine (not in the cloud)\n",
    "# Set to \"cloud\" when running on cloud infrastructure (e.g., AWS us-west-2)\n",
    "ENVIRONMENT = \"cloud\"  # Options: \"local\" or \"cloud\"\n",
    "\n",
    "# Optional: Add a description for this run (e.g., your location, machine specs)\n",
    "RUN_DESCRIPTION = \"CryoCloud\"\n",
    "\n",
    "# File to store timing results for comparison\n",
    "TIMINGS_FILE = Path(\"cloud_data_timings.json\")\n",
    "\n",
    "print(f\"ğŸ–¥ï¸  Running in '{ENVIRONMENT.upper()}' environment\")\n",
    "print(f\"ğŸ“ Description: {RUN_DESCRIPTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5e1c1",
   "metadata": {},
   "source": [
    "Set up timing dictionary and helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44da9087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T22:42:09.648457Z",
     "iopub.status.busy": "2025-12-17T22:42:09.648261Z",
     "iopub.status.idle": "2025-12-17T22:42:09.658600Z",
     "shell.execute_reply": "2025-12-17T22:42:09.657756Z",
     "shell.execute_reply.started": "2025-12-17T22:42:09.648439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary to store timings for current run\n",
    "timings = {\n",
    "    'open': {},\n",
    "    'spatial_subset_load': {},\n",
    "    'time_slice_load': {},\n",
    "    'timeseries_load': {},\n",
    "}\n",
    "\n",
    "# Constants for consistent test parameters\n",
    "SPATIAL_SUBSET_KWARGS = {\"time\": \"2001-01-02\", \"lat\": slice(10, 15), \"lon\": slice(-60, -55)}\n",
    "TIME_SLICE_KWARGS = {\"time\": \"2001-01-10\"}\n",
    "SPATIAL_POINT_KWARGS = {\"lat\": 45, \"lon\": -150, \"method\": \"nearest\"}\n",
    "N_FILES = 30\n",
    "\n",
    "def record_timing(category, method, elapsed_time):\n",
    "    \"\"\"Helper to record timing results (using perf_counter for precision).\"\"\"\n",
    "    timings[category][method] = elapsed_time\n",
    "    print(f\"  â±ï¸  {category} / {method}: {elapsed_time:.2f}s\")\n",
    "\n",
    "\n",
    "def benchmark_method(ds, method_name, n_files=N_FILES):\n",
    "    \"\"\"Run all benchmark tests on a dataset and record timings.\"\"\"\n",
    "    \n",
    "    start = perf_counter()\n",
    "    data = ds['Tair'].sel(**SPATIAL_SUBSET_KWARGS).load()\n",
    "    record_timing('spatial_subset_load', method_name, perf_counter() - start)\n",
    "\n",
    "    start = perf_counter()\n",
    "    data = ds['Tair'].sel(**TIME_SLICE_KWARGS).load()\n",
    "    record_timing('time_slice_load', method_name, perf_counter() - start)\n",
    "\n",
    "    start = perf_counter()\n",
    "    data = ds['Tair'].sel(**SPATIAL_POINT_KWARGS).isel(time=slice(0, n_files)).load()\n",
    "    record_timing('timeseries_load', method_name, perf_counter() - start)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def print_summary(method_name):\n",
    "    \"\"\"Print a summary of timings for a method.\"\"\"\n",
    "    total = sum(timings[op].get(method_name, 0) for op in timings.keys())\n",
    "    print(f\"\\n  â”Œ{'â”€'*50}\")\n",
    "    print(f\"  â”‚ ğŸ“‹ Summary for {method_name}\")\n",
    "    print(f\"  â”œ{'â”€'*50}\")\n",
    "    print(f\"  â”‚   Open:           {timings['open'].get(method_name, 0):>8.2f}s\")\n",
    "    print(f\"  â”‚   Spatial subset: {timings['spatial_subset_load'].get(method_name, 0):>8.2f}s\")\n",
    "    print(f\"  â”‚   Time slice:     {timings['time_slice_load'].get(method_name, 0):>8.2f}s\")\n",
    "    print(f\"  â”‚   Time series:    {timings['timeseries_load'].get(method_name, 0):>8.2f}s\")\n",
    "    print(f\"  â”œ{'â”€'*50}\")\n",
    "    print(f\"  â”‚   TOTAL:          {total:>8.2f}s\")\n",
    "    print(f\"  â””{'â”€'*50}\\n\")\n",
    "\n",
    "\n",
    "def load_all_timings():\n",
    "    \"\"\"Load all saved timing results from file.\"\"\"\n",
    "    if TIMINGS_FILE.exists():\n",
    "        with open(TIMINGS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_current_timings():\n",
    "    \"\"\"Save current timing results to file.\"\"\"\n",
    "    all_timings = load_all_timings()\n",
    "    \n",
    "    # Create entry for this run\n",
    "    run_entry = {\n",
    "        'description': RUN_DESCRIPTION,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'timings': timings\n",
    "    }\n",
    "    \n",
    "    # Store under environment key\n",
    "    all_timings[ENVIRONMENT] = run_entry\n",
    "    \n",
    "    with open(TIMINGS_FILE, 'w') as f:\n",
    "        json.dump(all_timings, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Timings saved to {TIMINGS_FILE}\")\n",
    "    print(f\"   Environment: {ENVIRONMENT}\")\n",
    "    print(f\"   Timestamp: {run_entry['timestamp']}\")\n",
    "\n",
    "\n",
    "def get_saved_environments():\n",
    "    \"\"\"Get list of environments with saved timings.\"\"\"\n",
    "    all_timings = load_all_timings()\n",
    "    return list(all_timings.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4e400",
   "metadata": {},
   "source": [
    "List the files available following this pattern on AWS S3 storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51c6f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T22:42:10.988440Z",
     "iopub.status.busy": "2025-12-17T22:42:10.988115Z",
     "iopub.status.idle": "2025-12-17T22:42:12.868050Z",
     "shell.execute_reply": "2025-12-17T22:42:12.867221Z",
     "shell.execute_reply.started": "2025-12-17T22:42:10.988417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://nasa-waterinsight/NLDAS3/forcing/daily/200101/NLDAS_FOR0010_D.A20010101.030.beta.nc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "nldas_files = fs.glob('s3://nasa-waterinsight/NLDAS3/forcing/daily/**/*.nc')\n",
    "nldas_files = sorted(['s3://'+f for f in nldas_files])\n",
    "nldas_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c60d24-3b36-4172-baaf-620d674e67ce",
   "metadata": {},
   "source": [
    "### Try opening file with xarray without any configuration\n",
    "\n",
    "Xarray does a lot of guessing about how to effectively open data files, but it doesn't usually get it right without any configuration for files located on the cloud. This is because it cannot know whether the files can be read anonymously or requires configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f8feb7-87a0-4055-a1f7-4ded7973c082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T22:42:16.444047Z",
     "iopub.status.busy": "2025-12-17T22:42:16.443628Z",
     "iopub.status.idle": "2025-12-17T22:42:16.446925Z",
     "shell.execute_reply": "2025-12-17T22:42:16.446011Z",
     "shell.execute_reply.started": "2025-12-17T22:42:16.444025Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will fail\n",
    "\n",
    "# xr.open_dataset(nldas_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e7449",
   "metadata": {},
   "source": [
    "### Opening archival data with fsspec + h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c84f267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T22:42:17.436984Z",
     "iopub.status.busy": "2025-12-17T22:42:17.436678Z",
     "iopub.status.idle": "2025-12-17T22:51:48.200609Z",
     "shell.execute_reply": "2025-12-17T22:51:48.199723Z",
     "shell.execute_reply.started": "2025-12-17T22:42:17.436959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing: fsspec + h5netcdf (default cache)\n",
      "  â±ï¸  open / fsspec_default_cache: 539.19s\n",
      "  â±ï¸  spatial_subset_load / fsspec_default_cache: 0.26s\n",
      "  â±ï¸  time_slice_load / fsspec_default_cache: 31.06s\n",
      "  â±ï¸  timeseries_load / fsspec_default_cache: 0.24s\n",
      "\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â”‚ ğŸ“‹ Summary for fsspec_default_cache\n",
      "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â”‚   Open:             539.19s\n",
      "  â”‚   Spatial subset:     0.26s\n",
      "  â”‚   Time slice:        31.06s\n",
      "  â”‚   Time series:        0.24s\n",
      "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â”‚   TOTAL:            570.76s\n",
      "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nğŸ“Š Testing: fsspec + h5netcdf (default cache)\")\n",
    "start = perf_counter()\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "file_objs = [fs.open(f) for f in nldas_files[:N_FILES]]\n",
    "ds = xr.open_mfdataset(file_objs, engine=\"h5netcdf\", combine=\"nested\", concat_dim=\"time\")\n",
    "record_timing('open', 'fsspec_default_cache', perf_counter() - start)\n",
    "\n",
    "benchmark_method(ds, 'fsspec_default_cache')\n",
    "print_summary('fsspec_default_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc41725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T22:54:07.277614Z",
     "iopub.status.busy": "2025-12-17T22:54:07.277261Z",
     "iopub.status.idle": "2025-12-17T22:54:07.367745Z",
     "shell.execute_reply": "2025-12-17T22:54:07.366819Z",
     "shell.execute_reply.started": "2025-12-17T22:54:07.277579Z"
    }
   },
   "outputs": [],
   "source": [
    "[f.close() for f in file_objs]\n",
    "fs.clear_instance_cache()\n",
    "del fs, file_objs, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c97f0",
   "metadata": {},
   "source": [
    "This took a lot of time to open the file. Let's look how we can speed that up by configuring the caching strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8645bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T22:54:08.266191Z",
     "iopub.status.busy": "2025-12-17T22:54:08.265901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing: fsspec + h5netcdf (block cache)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nğŸ“Š Testing: fsspec + h5netcdf (block cache)\")\n",
    "start = perf_counter()\n",
    "fsspec_caching = {\n",
    "    \"cache_type\": \"blockcache\",\n",
    "    \"block_size\": 1024 * 1024 * 8,\n",
    "}\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "file_objs = [fs.open(f, **fsspec_caching) for f in nldas_files[:N_FILES]]\n",
    "ds = xr.open_mfdataset(file_objs, engine=\"h5netcdf\", combine=\"nested\", concat_dim=\"time\")\n",
    "record_timing('open', 'fsspec_block_cache', perf_counter() - start)\n",
    "\n",
    "benchmark_method(ds, 'fsspec_block_cache')\n",
    "print_summary('fsspec_block_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5239ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f.close() for f in file_objs]\n",
    "fs.clear_instance_cache()\n",
    "del fs, file_objs, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8ae8f",
   "metadata": {},
   "source": [
    "### Opening archival data using VirtualiZarr + Icechunk\n",
    "\n",
    "Now, for the really cool part! Using [VirtualiZarr](https://virtualizarr.readthedocs.io/) + [Icechunk](https://icechunk.io/), we can rapidly open not just that file but all of the files included in the NLDAS3 dataset! In less than 2 seconds, we can have a lazy view of a dataset that contains 24 years of data. People will often use the term \"lazy loading\" when an operation loads metadata from a storage location, but does not load any actual data. Without the cloud-native adaptation virtual Zarr, it's not possible for a software library to determine how much data it should load from disk to get all the necessary metadata. Virtual Zarr is a faster, cheaper, and easier way to work with data on the cloud :rocket:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ca8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import icechunk\n",
    "import zarr\n",
    "import xarray as xr\n",
    "\n",
    "zarr.config.set({'threading.max_workers': 32, 'async.concurrency': 128})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nğŸ“Š Testing: VirtualiZarr + Icechunk\")\n",
    "start = perf_counter()\n",
    "storage = icechunk.s3_storage(\n",
    "    bucket='nasa-waterinsight',\n",
    "    prefix=f\"virtual-zarr-store/NLDAS-3-icechunk\",\n",
    "    region=\"us-west-2\",\n",
    "    anonymous=True,\n",
    ")\n",
    "\n",
    "chunk_url = \"s3://nasa-waterinsight/NLDAS3/forcing/daily/\"\n",
    "virtual_credentials = icechunk.containers_credentials({\n",
    "    chunk_url: icechunk.s3_anonymous_credentials()\n",
    "})\n",
    "\n",
    "repo = icechunk.Repository.open(\n",
    "    storage=storage,\n",
    "    authorize_virtual_chunk_access=virtual_credentials,\n",
    ")\n",
    "\n",
    "session = repo.readonly_session('main')\n",
    "ds = xr.open_zarr(session.store, consolidated=False, zarr_format=3, chunks={})\n",
    "record_timing('open', 'virtualzarr_icechunk', perf_counter() - start)\n",
    "\n",
    "benchmark_method(ds, 'virtualzarr_icechunk')\n",
    "print_summary('virtualzarr_icechunk')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c499d4a",
   "metadata": {},
   "source": [
    "### Save Timings for This Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the timing results for this environment\n",
    "save_current_timings()\n",
    "\n",
    "# Show what environments we have data for\n",
    "saved_envs = get_saved_environments()\n",
    "print(f\"\\nğŸ“ Saved timing data available for: {saved_envs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554035bf",
   "metadata": {},
   "source": [
    "## Local vs. Cloud Performance Comparison\n",
    "\n",
    "After running this notebook in both environments, run this section to see the performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_report():\n",
    "    \"\"\"Create a comprehensive comparison between local and cloud runs.\"\"\"\n",
    "    all_timings = load_all_timings()\n",
    "    \n",
    "    if len(all_timings) < 2:\n",
    "        missing = []\n",
    "        if 'local' not in all_timings:\n",
    "            missing.append('local')\n",
    "        if 'cloud' not in all_timings:\n",
    "            missing.append('cloud')\n",
    "        print(f\"âš ï¸  Need timing data from both environments!\")\n",
    "        print(f\"   Missing: {missing}\")\n",
    "        print(f\"   Available: {list(all_timings.keys())}\")\n",
    "        print(f\"\\n   To complete the comparison:\")\n",
    "        print(f\"   1. Run this notebook with ENVIRONMENT = 'local' on your local machine\")\n",
    "        print(f\"   2. Run this notebook with ENVIRONMENT = 'cloud' on cloud infrastructure (e.g., AWS us-west-2)\")\n",
    "        print(f\"   3. Copy the {TIMINGS_FILE} file between environments or manually combine results\")\n",
    "        return None\n",
    "    \n",
    "    return all_timings\n",
    "\n",
    "all_timings = create_comparison_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e19522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(all_timings):\n",
    "    \"\"\"Create a visual comparison of local vs cloud performance.\"\"\"\n",
    "    if all_timings is None:\n",
    "        return\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    methods = ['fsspec_default_cache', 'fsspec_block_cache', 'virtualzarr_icechunk']\n",
    "    method_labels = ['fsspec\\n(default)', 'fsspec\\n(block)', 'VirtualiZarr\\n+ Icechunk']\n",
    "    \n",
    "    # Calculate total times for each method\n",
    "    local_totals = []\n",
    "    cloud_totals = []\n",
    "    \n",
    "    for method in methods:\n",
    "        local_total = sum(\n",
    "            all_timings.get('local', {}).get('timings', {}).get(op, {}).get(method, 0)\n",
    "            for op in ['open', 'spatial_subset_load', 'time_slice_load', 'timeseries_load']\n",
    "        )\n",
    "        cloud_total = sum(\n",
    "            all_timings.get('cloud', {}).get('timings', {}).get(op, {}).get(method, 0)\n",
    "            for op in ['open', 'spatial_subset_load', 'time_slice_load', 'timeseries_load']\n",
    "        )\n",
    "        local_totals.append(local_total)\n",
    "        cloud_totals.append(cloud_total)\n",
    "    \n",
    "    # Create bar chart\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Total time comparison\n",
    "    bars1 = ax1.bar(x - width/2, local_totals, width, label='Local', color='#ff6b6b', alpha=0.8)\n",
    "    bars2 = ax1.bar(x + width/2, cloud_totals, width, label='Cloud (in-region)', color='#4ecdc4', alpha=0.8)\n",
    "    \n",
    "    ax1.set_ylabel('Total Time (seconds)', fontsize=12)\n",
    "    ax1.set_title('Total Execution Time: Local vs Cloud', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(method_labels, fontsize=10)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.1f}s',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.1f}s',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Speedup comparison\n",
    "    speedups = [l/c if c > 0 else 0 for l, c in zip(local_totals, cloud_totals)]\n",
    "    colors = ['#2ecc71' if s > 1 else '#e74c3c' for s in speedups]\n",
    "    bars3 = ax2.bar(x, speedups, color=colors, alpha=0.8)\n",
    "    ax2.axhline(y=1, color='gray', linestyle='--', alpha=0.7, label='No speedup')\n",
    "    ax2.set_ylabel('Speedup Factor (Local Time / Cloud Time)', fontsize=12)\n",
    "    ax2.set_title('Cloud Speedup by Method', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(method_labels, fontsize=10)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, speedup in zip(bars3, speedups):\n",
    "        height = bar.get_height()\n",
    "        ax2.annotate(f'{speedup:.1f}x',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('local_vs_cloud_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“Š Chart saved to 'local_vs_cloud_comparison.png'\")\n",
    "\n",
    "if all_timings:\n",
    "    plot_comparison(all_timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_key_insights(all_timings):\n",
    "    \"\"\"Print key insights from the comparison.\"\"\"\n",
    "    if all_timings is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ’¡ KEY INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate average speedup\n",
    "    speedups = []\n",
    "    methods = ['fsspec_default_cache', 'fsspec_block_cache',  'virtualzarr_icechunk']\n",
    "    \n",
    "    for method in methods:\n",
    "        local_total = sum(\n",
    "            all_timings.get('local', {}).get('timings', {}).get(op, {}).get(method, 0)\n",
    "            for op in ['open', 'spatial_subset_load', 'time_slice_load', 'timeseries_load']\n",
    "        )\n",
    "        cloud_total = sum(\n",
    "            all_timings.get('cloud', {}).get('timings', {}).get(op, {}).get(method, 0)\n",
    "            for op in ['open', 'spatial_subset_load', 'time_slice_load', 'timeseries_load']\n",
    "        )\n",
    "        if cloud_total > 0:\n",
    "            speedups.append((method, local_total / cloud_total, local_total, cloud_total))\n",
    "    \n",
    "    if speedups:\n",
    "        avg_speedup = sum(s[1] for s in speedups) / len(speedups)\n",
    "        max_speedup = max(speedups, key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Average speedup from data-proximate computing: {avg_speedup:.1f}x\")\n",
    "        print(f\"ğŸ† Best speedup: {max_speedup[0]} ({max_speedup[1]:.1f}x faster on cloud)\")\n",
    "        \n",
    "        # Time saved\n",
    "        total_local = sum(s[2] for s in speedups)\n",
    "        total_cloud = sum(s[3] for s in speedups)\n",
    "        time_saved = total_local - total_cloud\n",
    "        \n",
    "        print(f\"\\nâ±ï¸  Total time (all methods):\")\n",
    "        print(f\"   Local:  {total_local:.1f} seconds\")\n",
    "        print(f\"   Cloud:  {total_cloud:.1f} seconds\")\n",
    "        print(f\"   Saved:  {time_saved:.1f} seconds ({(time_saved/total_local)*100:.0f}% reduction)\")\n",
    "        \n",
    "        print(\"\\nğŸ¯ Key takeaway: Running your code on cloud infrastructure\")\n",
    "        print(\"   co-located with your data can dramatically improve performance!\")\n",
    "\n",
    "if all_timings:\n",
    "    print_key_insights(all_timings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7d59c",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "- When working on the cloud, try to find computing resources that are \"in-region\" to the data you're working with.\n",
    "- File formats matter - consider using virtual Zarr if your data are not already \"cloud-optimized\".\n",
    "- File access patterns matter - the default arguments for reading data from the cloud may be very slow! You can customize the configuration for better performance.\n",
    "- **Data-proximate computing can provide dramatic speedups compared to working locally!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f41f4-1541-4524-a392-2ea070c1632a",
   "metadata": {},
   "source": [
    "## How to learn more\n",
    "\n",
    "There are several communities oriented towards scalable open science, here are a couple entry points:\n",
    "\n",
    "- [Post questions on the Pangeo Discourse](https://discourse.pangeo.io/)\n",
    "- [Join CNG and ask questions on their slack](https://cloudnativegeo.org/join/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8db4b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Cloud-Optimized Geospatial Formats Guide](https://guide.cloudnativegeo.org/)\n",
    "- [Xarray Tutorial - Zarr in Cloud Object Storage](https://tutorial.xarray.dev/intermediate/remote_data/cmip6-cloud.html)\n",
    "- [Xarray Tutorial - Access Patterns to Remote Data with fsspec](https://tutorial.xarray.dev/intermediate/remote_data/cmip6-cloud.html)\n",
    "- [ICESAT-2 Cloud Computing Tutorial](https://icesat-2-2024.hackweek.io/tutorials/cloud-computing/00-goals-and-outline.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e819dde-68bc-4dbd-82f7-fc1f544b8b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
